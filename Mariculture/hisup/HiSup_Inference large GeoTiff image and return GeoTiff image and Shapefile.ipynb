{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wvlq4PekSIyJ"
   },
   "source": [
    "# HiSup - Inference large GeoTiff image and return prediction mask in GeoTiff and extracted buildings in Shapefile\n",
    "\n",
    "This work is based on https://github.com/SarahwXU/HiSup.<br> \n",
    "I added several blocks to enable creating prediction results with spatial information.Feel free to reuse it for your own application.<br>\n",
    "Here you can use fine-tuned model from your own dataset.<br>\n",
    "To run this Jupyter notebook, you need to preinstall the required packages as instructed by HiSup GitHub.<br>\n",
    "Change the path for your own application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "jnVks4XDQdto"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/qiaowenjiao/HiSup\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'shapely'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-a9d9d15a1159>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mscripts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdemo\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minference_no_patching\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minference_with_patching\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mhisup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mhisup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetector\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_pretrained_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/qiaowenjiao/HiSup/scripts/demo.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mhisup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mhisup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetector\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_pretrained_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mhisup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbuild_transform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mhisup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mto_single_device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mhisup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvisualizer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mshow_polygons\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/qiaowenjiao/HiSup/hisup/dataset/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtrain_dataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTrainDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbuild_train_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuild_test_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuild_train_dataset_multi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtest_dataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTestDatasetWithAnnotations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/qiaowenjiao/HiSup/hisup/dataset/train_dataset.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mskimage\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpycocotools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoco\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCOCO\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mshapely\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeometry\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPolygon\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataloader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdefault_collate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'shapely'"
     ]
    }
   ],
   "source": [
    "# Define the path\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import pathlib\n",
    "\n",
    "path = os.getcwd()\n",
    "print(path)\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "sys.path.append(os.path.abspath('.'))\n",
    "\n",
    "from scripts.demo import inference_no_patching, inference_with_patching\n",
    "from hisup.config import cfg\n",
    "from hisup.detector import get_pretrained_model\n",
    "from skimage import io\n",
    "import matplotlib.pyplot as plt\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Collecting pycocotools\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/e3/0a/f2c7565edb94530fa9205779a537df50bf315b928de095a0e3aa7fbdb366/pycocotools-2.0.7.tar.gz (24 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: matplotlib>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from pycocotools) (3.3.3)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pycocotools) (1.19.4)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.1.0->pycocotools) (8.4.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.1.0->pycocotools) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.1.0->pycocotools) (1.3.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.1.0->pycocotools) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.1.0->pycocotools) (2.8.1)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from cycler>=0.10->matplotlib>=2.1.0->pycocotools) (1.15.0)\n",
      "Building wheels for collected packages: pycocotools\n",
      "  Building wheel for pycocotools (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pycocotools: filename=pycocotools-2.0.7-cp36-cp36m-linux_x86_64.whl size=267170 sha256=cea1eed8eac2dca5b10f1c09384e2077592a624f574f225993c85bd8bd1dd279\n",
      "  Stored in directory: /root/.cache/pip/wheels/4c/f1/69/7ca81b60943a36f77e60d81e5e50047e9930a7d2764a5b1535\n",
      "Successfully built pycocotools\n",
      "Installing collected packages: pycocotools\n",
      "Successfully installed pycocotools-2.0.7\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pycocotools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Z4QJ82NzWtH"
   },
   "source": [
    "### Inference Functions from HiSup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1BWDeE58zZz8"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "from skimage import io\n",
    "from hisup.config import cfg\n",
    "from hisup.detector import get_pretrained_model\n",
    "from hisup.dataset.build import build_transform\n",
    "from hisup.utils.comm import to_single_device\n",
    "from hisup.utils.visualizer import show_polygons\n",
    "\n",
    "import scipy.ndimage\n",
    "from tqdm import tqdm\n",
    "from shapely.geometry import Polygon\n",
    "from skimage.measure import label, regionprops\n",
    "from hisup.utils.polygon import generate_polygon, juncs_in_bbox\n",
    "from hisup.utils.visualizer import viz_inria\n",
    "\n",
    "from hisup.config import cfg\n",
    "from hisup.detector import BuildingDetector\n",
    "from hisup.dataset import build_train_dataset\n",
    "from hisup.utils.comm import to_single_device\n",
    "from hisup.solver import make_lr_scheduler, make_optimizer\n",
    "from hisup.utils.logger import setup_logger\n",
    "from hisup.utils.miscellaneous import save_config\n",
    "from hisup.utils.metric_logger import MetricLogger\n",
    "from hisup.utils.checkpoint import DetectronCheckpointer\n",
    "\n",
    "def inference_single(cfg, model, image, device):\n",
    "    \n",
    "    transform = build_transform(cfg)\n",
    "    \n",
    "    h_stride, w_stride = 400, 400\n",
    "    h_crop, w_crop = cfg.DATASETS.ORIGIN.HEIGHT, cfg.DATASETS.ORIGIN.WIDTH\n",
    "    h_img, w_img, _ = image.shape\n",
    "    h_grids = max(h_img - h_crop + h_stride - 1, 0) // h_stride + 1\n",
    "    w_grids = max(w_img - w_crop + w_stride - 1, 0) // w_stride + 1\n",
    "    pred_whole_img = np.zeros([h_img, w_img], dtype=np.float32)\n",
    "    count_mat = np.zeros([h_img, w_img])\n",
    "    juncs_whole_img = []\n",
    "    \n",
    "    patch_weight = np.ones((h_crop + 2, w_crop + 2))\n",
    "    patch_weight[0,:] = 0\n",
    "    patch_weight[-1,:] = 0\n",
    "    patch_weight[:,0] = 0\n",
    "    patch_weight[:,-1] = 0\n",
    "    \n",
    "    patch_weight = scipy.ndimage.distance_transform_edt(patch_weight)\n",
    "    patch_weight = patch_weight[1:-1,1:-1]\n",
    "\n",
    "    for h_idx in tqdm(range(h_grids), desc='processing on image'):\n",
    "        for w_idx in range(w_grids):\n",
    "            y1 = h_idx * h_stride\n",
    "            x1 = w_idx * w_stride\n",
    "            y2 = min(y1 + h_crop, h_img)\n",
    "            x2 = min(x1 + w_crop, w_img)\n",
    "            y1 = max(y2 - h_crop, 0)\n",
    "            x1 = max(x2 - w_crop, 0)\n",
    "            \n",
    "            crop_img = image[y1:y2, x1:x2, :]\n",
    "            crop_img_tensor = transform(crop_img.astype(float))[None].to(device)\n",
    "            \n",
    "            meta = {\n",
    "                'height': crop_img.shape[0],\n",
    "                'width': crop_img.shape[1],\n",
    "                'pos': [x1, y1, x2, y2]\n",
    "            }\n",
    "\n",
    "            with torch.no_grad():\n",
    "                output, _ = model(crop_img_tensor, [meta])\n",
    "                output = to_single_device(output, 'cpu')\n",
    "\n",
    "            juncs_pred = output['juncs_pred'][0]\n",
    "            mask_pred = output['mask_pred'][0]\n",
    "            juncs_pred += [x1, y1]\n",
    "            juncs_whole_img.extend(juncs_pred.tolist())\n",
    "            mask_pred *= patch_weight\n",
    "            pred_whole_img += np.pad(mask_pred,\n",
    "                                ((int(y1), int(pred_whole_img.shape[0] - y2)),\n",
    "                                (int(x1), int(pred_whole_img.shape[1] - x2))))\n",
    "            count_mat[y1:y2, x1:x2] += patch_weight\n",
    "\n",
    "    juncs_whole_img = np.array(juncs_whole_img)\n",
    "    pred_whole_img = pred_whole_img / count_mat\n",
    "\n",
    "    return juncs_whole_img, pred_whole_img\n",
    "\n",
    "\n",
    "def inference_in_batches(cfg, model, image_patches, device):\n",
    "\n",
    "    for i in range(image_patches.shape[0]):\n",
    "        \n",
    "        image = image_patches[i]\n",
    "        juncs_whole_img_single, pred_whole_img_single = inference_single(cfg, model, image, device)\n",
    "        juncs_whole_img_single_ex = np.expand_dims(juncs_whole_img_single, axis=0)\n",
    "        pred_whole_img_single_ex = np.expand_dims(pred_whole_img_single, axis=0)\n",
    "\n",
    "        if i == 0:\n",
    "            juncs_whole_img_all = juncs_whole_img_single_ex\n",
    "            pred_whole_img_all = pred_whole_img_single_ex\n",
    "\n",
    "        else:\n",
    "            juncs_whole_img_all = np.concatenate((juncs_whole_img_all, juncs_whole_img_single_ex), axis=0)\n",
    "            pred_whole_img_all = np.concatenate((pred_whole_img_all, pred_whole_img_single_ex), axis=0)\n",
    "\n",
    "    return juncs_whole_img_all, pred_whole_img_all\n",
    "\n",
    "\n",
    "def convert_mask_to_polygons(juncs_whole_img, pred_whole_img):\n",
    "    # match junction and seg results\n",
    "    polygons = []\n",
    "    props = regionprops(label(pred_whole_img > 0.5))\n",
    "    for prop in tqdm(props, leave=False, desc='polygon generation'):\n",
    "        y1, x1, y2, x2 = prop.bbox\n",
    "        bbox = [x1, y1, x2, y2]\n",
    "        select_juncs = juncs_in_bbox(bbox, juncs_whole_img, expand=8)\n",
    "        poly, juncs_sa, _, _, juncs_index = generate_polygon(prop, pred_whole_img, \\\n",
    "                                                                    select_juncs, pid=0, test_inria=True)\n",
    "        if juncs_sa.shape[0] == 0:\n",
    "            continue\n",
    "        \n",
    "        if len(juncs_index) == 1:\n",
    "            polygons.append(Polygon(poly))\n",
    "        else:\n",
    "            poly_ = Polygon(poly[juncs_index[0]], \\\n",
    "                            [poly[idx] for idx in juncs_index[1:]])\n",
    "            polygons.append(poly_)\n",
    "    \n",
    "    return polygons\n",
    "\n",
    "def get_pretrained_model_FT(cfg, dataset, device, path_model, pretrained=True):\n",
    "    \n",
    "    model = BuildingDetector(cfg, test=True)\n",
    "    # if pretrained:\n",
    "    #     url = PRETRAINED[dataset]\n",
    "    #     state_dict = torch.hub.load_state_dict_from_url(url, map_location=device, progress=True)\n",
    "    # state_dict = {k[7:]:v for k,v in state_dict['model'].items() if k[0:7] == 'module.'}\n",
    "    # model.load_state_dict(state_dict)\n",
    "    # state_dict = model.load_state_dict(path_model)\n",
    "    # state_dict = {k[7:]:v for k,v in state_dict['model'].items() if k[0:7] == 'module.'}\n",
    "    # model.load_state_dict(state_dict)\n",
    "    state_dict = torch.load(path_model)\n",
    "    model.load_state_dict(state_dict[\"model\"])\n",
    "    model = model.eval()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = os.path.join(path, \"data\")\n",
    "path_test = os.path.join(path_data, \"outputs\")\n",
    "img_path = \"path to your geotif file\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IQ9GXoTboIV6"
   },
   "source": [
    "### Crop a geotiff raster whose width and height can be divided by 512 (may be optional), and save it as a new geotiff raster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14799,
     "status": "ok",
     "timestamp": 1684740502496,
     "user": {
      "displayName": "Yunya Gao",
      "userId": "05443726764578958815"
     },
     "user_tz": -120
    },
    "id": "UIJfy9Pak4Ge",
    "outputId": "8d846943-9de6-4909-d447-1e935422cbb2"
   },
   "outputs": [],
   "source": [
    "import rasterio\n",
    "from rasterio.windows import Window\n",
    "from patchify import patchify, unpatchify\n",
    "\n",
    "src = rasterio.open(img_path)\n",
    "width = src.width\n",
    "height = src.height\n",
    "\n",
    "patchsize = 512\n",
    "width_new = int(width / patchsize) * patchsize\n",
    "height_new = int(height / patchsize) * patchsize\n",
    "print(width, height, width_new, height_new)\n",
    "\n",
    "xmin, xmax = 0, width_new\n",
    "ymin, ymax = 0, height_new\n",
    "xoff, yoff = 1, 1\n",
    "\n",
    "# Create a Window and calculate the transform from the source dataset    \n",
    "window = Window(xoff, yoff, width_new, height_new)\n",
    "transform = src.window_transform(window)\n",
    "\n",
    "profile = src.profile\n",
    "print(profile)\n",
    "profile.update({\n",
    "    'height': height_new, # it can be confusing\n",
    "    'width': width_new,\n",
    "    'transform': transform})\n",
    "print(profile)\n",
    "\n",
    "img_out_path = os.path.join(path_test, \"image_test_cropped.tif\")\n",
    "with rasterio.open(img_out_path, 'w', **profile) as dst:\n",
    "    # Read the data from the window and write it to the output raster\n",
    "    dst.write(src.read(window=window))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DLF4eUXsktnU"
   },
   "source": [
    "### Load pretrained model from your data or Infria or Crowdai data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12982,
     "status": "ok",
     "timestamp": 1684740581963,
     "user": {
      "displayName": "Yunya Gao",
      "userId": "05443726764578958815"
     },
     "user_tz": -120
    },
    "id": "73hoFK9Niac5",
    "outputId": "f82542d0-6a10-43be-b672-82a1ea1b7dde"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "dataset = 'crowdai' # crowdai inria\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "img_path = os.path.join(path_test, \"image_test_cropped.tif\")\n",
    "image = io.imread(img_path)[:, :, :3]\n",
    "\n",
    "H, W = patchsize, patchsize # patchsize = 512\n",
    "cfg.DATASETS.ORIGIN.HEIGHT = 512 if H > 512 else H\n",
    "cfg.DATASETS.ORIGIN.WIDTH = 512 if W > 512 else W\n",
    "\n",
    "# here I \n",
    "# load your pretrained model\n",
    "path_model = \"path to pretrained model(.pth)\"\n",
    "\n",
    "model = get_pretrained_model_FT(cfg, dataset, device, path_model, pretrained=True)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference, creating mask raster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.ndimage\n",
    "from tqdm import tqdm\n",
    "from shapely.geometry import Polygon\n",
    "from skimage.measure import label, regionprops\n",
    "from hisup.utils.polygon import generate_polygon, juncs_in_bbox\n",
    "from hisup.utils.visualizer import viz_inria\n",
    "\n",
    "transform = build_transform(cfg)\n",
    "\n",
    "h_stride, w_stride = 400, 400\n",
    "h_crop, w_crop = cfg.DATASETS.ORIGIN.HEIGHT, cfg.DATASETS.ORIGIN.WIDTH\n",
    "h_img, w_img, _ = image.shape\n",
    "h_grids = max(h_img - h_crop + h_stride - 1, 0) // h_stride + 1\n",
    "w_grids = max(w_img - w_crop + w_stride - 1, 0) // w_stride + 1\n",
    "pred_whole_img = np.zeros([h_img, w_img], dtype=np.float32)\n",
    "count_mat = np.zeros([h_img, w_img])\n",
    "juncs_whole_img = []\n",
    "\n",
    "patch_weight = np.ones((h_crop + 2, w_crop + 2))\n",
    "patch_weight[0,:] = 0\n",
    "patch_weight[-1,:] = 0\n",
    "patch_weight[:,0] = 0\n",
    "patch_weight[:,-1] = 0\n",
    "\n",
    "patch_weight = scipy.ndimage.distance_transform_edt(patch_weight)\n",
    "patch_weight = patch_weight[1:-1,1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for h_idx in tqdm(range(h_grids), desc='processing on image'):\n",
    "    for w_idx in range(w_grids):\n",
    "        y1 = h_idx * h_stride\n",
    "        x1 = w_idx * w_stride\n",
    "        y2 = min(y1 + h_crop, h_img)\n",
    "        x2 = min(x1 + w_crop, w_img)\n",
    "        y1 = max(y2 - h_crop, 0)\n",
    "        x1 = max(x2 - w_crop, 0)\n",
    "\n",
    "        crop_img = image[y1:y2, x1:x2, :]\n",
    "        crop_img_tensor = transform(crop_img.astype(float))[None].to(device)\n",
    "\n",
    "        meta = {\n",
    "            'height': crop_img.shape[0],\n",
    "            'width': crop_img.shape[1],\n",
    "            'pos': [x1, y1, x2, y2]\n",
    "        }\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output, _ = model(crop_img_tensor, [meta])\n",
    "            output = to_single_device(output, 'cpu')\n",
    "\n",
    "        juncs_pred = output['juncs_pred'][0]\n",
    "        mask_pred = output['mask_pred'][0]\n",
    "        juncs_pred += [x1, y1]\n",
    "        juncs_whole_img.extend(juncs_pred.tolist())\n",
    "        mask_pred *= patch_weight\n",
    "        pred_whole_img += np.pad(mask_pred,\n",
    "                            ((int(y1), int(pred_whole_img.shape[0] - y2)),\n",
    "                            (int(x1), int(pred_whole_img.shape[1] - x2))))\n",
    "        count_mat[y1:y2, x1:x2] += patch_weight\n",
    "\n",
    "juncs_whole_img = np.array(juncs_whole_img)\n",
    "pred_whole_img = pred_whole_img / count_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save predicted masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save predicted mask and shapefile with geoinformation\n",
    "\n",
    "import pathlib\n",
    "import geopandas as gpd\n",
    "import rasterio\n",
    "\n",
    "path_out_img = os.path.join(path_test, \"Pred_Mask\")\n",
    "path_out_shp = os.path.join(path_test, \"Pred_Shp\")\n",
    "\n",
    "pathlib.Path(path_out_img).mkdir(parents=True, exist_ok=True) \n",
    "pathlib.Path(path_out_shp).mkdir(parents=True, exist_ok=True) \n",
    "\n",
    "# https://geobgu.xyz/py/rasterio.html\n",
    "with rasterio.open(img_path) as src:\n",
    "    ras_meta = src.profile\n",
    "    crs = src.crs # for vector\n",
    "    ras_meta[\"count\"] = 1 # for raster\n",
    "    ras_meta[\"dtype\"] = \"float32\" \n",
    "\n",
    "pred_whole_img_ = np.expand_dims(pred_whole_img[...], axis=0)\n",
    "\n",
    "path_out_img = \"path to output mask (.tif)\"\n",
    "with rasterio.open(path_out_img, 'w', **ras_meta) as dst:\n",
    "    dst.write(pred_whole_img_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict polygons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# match junction and seg results\n",
    "polygons = []\n",
    "thres = 0.5 # it can be an important variable, 0.5 may not be always the best option.\n",
    "\n",
    "props = regionprops(label(pred_whole_img > thres))\n",
    "for prop in tqdm(props, leave=False, desc='polygon generation'):\n",
    "    y1, x1, y2, x2 = prop.bbox\n",
    "    bbox = [x1, y1, x2, y2]\n",
    "    select_juncs = juncs_in_bbox(bbox, juncs_whole_img, expand=8)\n",
    "    poly, juncs_sa, _, _, juncs_index = generate_polygon(prop, pred_whole_img, select_juncs, pid=0, test_inria=True)\n",
    "                                                                \n",
    "    if juncs_sa.shape[0] == 0:\n",
    "        continue\n",
    "\n",
    "    if len(juncs_index) == 1:\n",
    "        polygons.append(Polygon(poly))\n",
    "    else:\n",
    "        poly_ = Polygon(poly[juncs_index[0]], \\\n",
    "                        [poly[idx] for idx in juncs_index[1:]])\n",
    "        polygons.append(poly_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add geoinformation to the created polygons\n",
    "\n",
    "from shapely.affinity import scale\n",
    "from shapely.ops import transform\n",
    "from shapely.geometry import Polygon\n",
    "import rasterio\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from geopandas import GeoSeries\n",
    "\n",
    "img_path = os.path.join(path_test, \"image_test_cropped.tif\")\n",
    "src = rasterio.open(img_path)\n",
    "left, bottom = src.bounds.left, src.bounds.bottom\n",
    "    \n",
    "# it is important to set the resolution when creating the final polygons in shapefile\n",
    "resolution = 0.1\n",
    "\n",
    "for poly in polygons:\n",
    "\n",
    "    x, y = poly.exterior.coords.xy\n",
    "    x_ = x.tolist()\n",
    "    y_ = y.tolist()\n",
    "\n",
    "    x__ = [i*resolution + left for i in x_]\n",
    "    y__ = [j*resolution + bottom for j in y_]\n",
    "    \n",
    "    poly_geo = Polygon(zip(x__, y__))\n",
    "    polygons_geo.append(poly_geo)\n",
    "\n",
    "len(polygons_geo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save polygons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polygons_gpd = gpd.GeoSeries(polygons_geo)\n",
    "\n",
    "origin = ((src.bounds.left + src.bounds.right)/2, (src.bounds.top + src.bounds.bottom)/2)\n",
    "flip = GeoSeries.scale(polygons_gpd, xfact=1.0, yfact=-1.0, zfact=0, origin=origin) \n",
    "\n",
    "path_shp_out = \"output shapefile (.shp)\"\n",
    "gdf = gpd.GeoDataFrame(crs=crs, geometry=flip)\n",
    "gdf.to_file(os.path.join(path_out_shp, path_shp_out), driver='ESRI Shapefile')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize polygons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flip\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axs = plt.subplots()\n",
    "fig.set_size_inches(25, 25)\n",
    "axs.set_aspect('equal', 'datalim')\n",
    "\n",
    "for geom in flip:   \n",
    "    xs, ys = geom.exterior.xy    \n",
    "    axs.fill(xs, ys, alpha=0.5, fc='r', ec='none')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "eqsmMLeE-NgY",
    "9iqFVND1pjRS",
    "IQ9GXoTboIV6",
    "AB73bX17oNwV",
    "DLF4eUXsktnU",
    "Eqw_MdG3u6FZ",
    "Vip7Q03Tw13A"
   ],
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
